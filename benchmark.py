#!/usr/bin/env python3
"""
pyFileIndexer æ€§èƒ½æµ‹è¯•è„šæœ¬

å®Œå…¨é€šè¿‡ CLI è°ƒç”¨ pyFileIndexer è¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œä¸æ¶‰åŠå†…éƒ¨é€»è¾‘ã€‚
æµ‹è¯•åŒ…æ‹¬æ‰«ææ€§èƒ½ã€æ•°æ®åº“æ“ä½œæ€§èƒ½ã€ç³»ç»Ÿèµ„æºä½¿ç”¨ç­‰ã€‚
"""

import argparse
import json
import psutil
import random
import shutil
import sqlite3
import subprocess
import tempfile
import threading
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple


@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""

    small_files: int = 100
    medium_files: int = 1000
    large_files: int = 5000
    file_size_range: Tuple[int, int] = (1024, 10 * 1024 * 1024)  # 1KB - 10MB
    duplicate_ratio: float = 0.1  # 10% é‡å¤æ–‡ä»¶
    machine_name: str = "benchmark-test"
    test_rounds: int = 3  # æ¯ä¸ªæµ‹è¯•è¿è¡Œæ¬¡æ•°


@dataclass
class ResourceMetrics:
    """ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""

    timestamp: float
    cpu_percent: float
    memory_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float


@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""

    test_name: str
    file_count: int
    total_size_mb: float
    scan_time_seconds: float
    files_per_second: float
    mb_per_second: float
    db_size_mb: float
    db_records: int
    resource_metrics: List[ResourceMetrics]
    cli_output: str


class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self, interval: float = 0.5):
        self.interval = interval
        self.metrics: List[ResourceMetrics] = []
        self.monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None

    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.metrics.clear()
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()

    def stop(self) -> List[ResourceMetrics]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æŒ‡æ ‡"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        return self.metrics.copy()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        process = psutil.Process()

        # å°è¯•è·å– I/O è®¡æ•°å™¨ï¼Œå¦‚æœä¸æ”¯æŒåˆ™ä½¿ç”¨ 0
        try:
            last_io = process.io_counters()
            io_supported = True
        except (AttributeError, OSError):
            last_io = None
            io_supported = False

        while self.monitoring:
            disk_io_read_mb = 0
            disk_io_write_mb = 0

            if io_supported:
                try:
                    current_io = process.io_counters()
                    disk_io_read_mb = (
                        (current_io.read_bytes - last_io.read_bytes) / 1024 / 1024
                    )
                    disk_io_write_mb = (
                        (current_io.write_bytes - last_io.write_bytes) / 1024 / 1024
                    )
                    last_io = current_io
                except (AttributeError, OSError):
                    io_supported = False

            metric = ResourceMetrics(
                timestamp=time.time(),
                cpu_percent=process.cpu_percent(),
                memory_mb=process.memory_info().rss / 1024 / 1024,
                disk_io_read_mb=disk_io_read_mb,
                disk_io_write_mb=disk_io_write_mb,
            )

            self.metrics.append(metric)
            time.sleep(self.interval)


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""

    @staticmethod
    def create_test_files(
        base_dir: Path, file_count: int, config: TestConfig
    ) -> Dict[str, any]:
        """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
        base_dir.mkdir(parents=True, exist_ok=True)

        files_info = {
            "total_files": file_count,
            "total_size": 0,
            "duplicate_files": 0,
            "file_types": {"text": 0, "binary": 0, "empty": 0},
        }

        # ç”ŸæˆåŸºç¡€æ–‡ä»¶
        duplicate_count = int(file_count * config.duplicate_ratio)
        unique_count = file_count - duplicate_count

        # åˆ›å»ºå”¯ä¸€æ–‡ä»¶
        for i in range(unique_count):
            file_path = base_dir / f"file_{i:06d}.txt"
            file_type, size = TestDataGenerator._create_single_file(file_path, config)
            files_info["total_size"] += size
            files_info["file_types"][file_type] += 1

        # åˆ›å»ºé‡å¤æ–‡ä»¶ï¼ˆå¤åˆ¶å·²æœ‰æ–‡ä»¶ï¼‰
        for i in range(duplicate_count):
            source_idx = random.randint(
                0, min(unique_count - 1, 50)
            )  # ä»å‰50ä¸ªæ–‡ä»¶ä¸­é€‰æ‹©
            source_file = base_dir / f"file_{source_idx:06d}.txt"
            if source_file.exists():
                duplicate_file = base_dir / f"duplicate_{i:06d}.txt"
                shutil.copy2(source_file, duplicate_file)
                files_info["total_size"] += source_file.stat().st_size
                files_info["duplicate_files"] += 1
                files_info["file_types"]["text"] += 1

        return files_info

    @staticmethod
    def _create_single_file(file_path: Path, config: TestConfig) -> Tuple[str, int]:
        """åˆ›å»ºå•ä¸ªæ–‡ä»¶"""
        min_size, max_size = config.file_size_range

        # å†³å®šæ–‡ä»¶ç±»å‹å’Œå¤§å°
        rand = random.random()
        if rand < 0.1:  # 10% ç©ºæ–‡ä»¶
            file_path.touch()
            return "empty", 0
        elif rand < 0.3:  # 20% äºŒè¿›åˆ¶æ–‡ä»¶
            size = random.randint(
                min_size, min(max_size, 1024 * 1024)
            )  # äºŒè¿›åˆ¶æ–‡ä»¶ä¸è¶…è¿‡1MB
            content = bytes([random.randint(0, 255) for _ in range(size)])
            file_path.write_bytes(content)
            return "binary", size
        else:  # 70% æ–‡æœ¬æ–‡ä»¶
            size = random.randint(min_size, max_size)
            lines = []
            current_size = 0
            while current_size < size:
                line = f"Line {len(lines):06d}: " + "x" * random.randint(10, 100) + "\n"
                lines.append(line)
                current_size += len(line.encode())

            content = "".join(lines)
            file_path.write_text(content)
            return "text", len(content.encode())


class BenchmarkRunner:
    """æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config: TestConfig):
        self.config = config
        self.project_root = Path(__file__).parent
        self.temp_dir: Optional[Path] = None
        self.results: List[PerformanceResult] = []

    def run_all_benchmarks(self) -> List[PerformanceResult]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ pyFileIndexer æ€§èƒ½æµ‹è¯•")
        print(f"æµ‹è¯•é…ç½®: {self.config}")
        print("-" * 60)

        try:
            self.temp_dir = Path(tempfile.mkdtemp(prefix="pyfileindexer_benchmark_"))
            print(f"æµ‹è¯•ç›®å½•: {self.temp_dir}")

            # å°è§„æ¨¡æµ‹è¯•
            self._run_scale_test("small", self.config.small_files)

            # ä¸­è§„æ¨¡æµ‹è¯•
            self._run_scale_test("medium", self.config.medium_files)

            # å¤§è§„æ¨¡æµ‹è¯•
            self._run_scale_test("large", self.config.large_files)

            # å¢é‡æ‰«ææµ‹è¯•
            self._run_incremental_test()

            # ä¿®æ”¹æ–‡ä»¶æµ‹è¯•
            self._run_modification_test()

        finally:
            if self.temp_dir and self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)

        return self.results

    def _run_scale_test(self, scale_name: str, file_count: int):
        """è¿è¡Œç‰¹å®šè§„æ¨¡çš„æµ‹è¯•"""
        print(f"\nğŸ“Š è¿è¡Œ {scale_name} è§„æ¨¡æµ‹è¯• ({file_count} æ–‡ä»¶)")

        test_dir = self.temp_dir / f"test_{scale_name}"
        db_path = self.temp_dir / f"test_{scale_name}.db"

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        print("  ç”Ÿæˆæµ‹è¯•æ–‡ä»¶...")
        files_info = TestDataGenerator.create_test_files(
            test_dir, file_count, self.config
        )
        print(
            f"  åˆ›å»ºäº† {files_info['total_files']} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {files_info['total_size'] / 1024 / 1024:.2f} MB"
        )

        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        for round_num in range(self.config.test_rounds):
            print(f"  ç¬¬ {round_num + 1}/{self.config.test_rounds} è½®æµ‹è¯•...")

            # åˆ é™¤ä¹‹å‰çš„æ•°æ®åº“
            if db_path.exists():
                db_path.unlink()

            result = self._run_single_scan(
                f"{scale_name}_round_{round_num + 1}",
                test_dir,
                db_path,
                files_info["total_files"],
                files_info["total_size"] / 1024 / 1024,
            )

            if result:
                self.results.append(result)
                print(f"    æ‰«ææ—¶é—´: {result.scan_time_seconds:.2f}s")
                print(f"    å¤„ç†é€Ÿåº¦: {result.files_per_second:.1f} æ–‡ä»¶/ç§’")
                print(f"    ååé‡: {result.mb_per_second:.2f} MB/ç§’")

    def _run_incremental_test(self):
        """è¿è¡Œå¢é‡æ‰«ææµ‹è¯•"""
        print("\nğŸ”„ è¿è¡Œå¢é‡æ‰«ææµ‹è¯•")

        test_dir = self.temp_dir / "test_incremental"
        db_path = self.temp_dir / "test_incremental.db"

        # é¦–æ¬¡æ‰«æ
        files_info = TestDataGenerator.create_test_files(
            test_dir, self.config.medium_files, self.config
        )

        print("  é¦–æ¬¡æ‰«æ...")
        first_result = self._run_single_scan(
            "incremental_first_scan",
            test_dir,
            db_path,
            files_info["total_files"],
            files_info["total_size"] / 1024 / 1024,
        )

        if first_result:
            self.results.append(first_result)

        # é‡å¤æ‰«æï¼ˆåº”è¯¥è·³è¿‡å¤§éƒ¨åˆ†æ–‡ä»¶ï¼‰
        print("  é‡å¤æ‰«æï¼ˆå¢é‡ï¼‰...")
        second_result = self._run_single_scan(
            "incremental_repeat_scan",
            test_dir,
            db_path,
            files_info["total_files"],
            files_info["total_size"] / 1024 / 1024,
        )

        if second_result:
            self.results.append(second_result)
            print(
                f"  æ€§èƒ½æå‡: {first_result.scan_time_seconds / second_result.scan_time_seconds:.2f}x"
            )

    def _run_modification_test(self):
        """è¿è¡Œæ–‡ä»¶ä¿®æ”¹æµ‹è¯•"""
        print("\nğŸ“ è¿è¡Œæ–‡ä»¶ä¿®æ”¹æµ‹è¯•")

        test_dir = self.temp_dir / "test_modification"
        db_path = self.temp_dir / "test_modification.db"

        # åˆ›å»ºåˆå§‹æ–‡ä»¶å¹¶æ‰«æ
        files_info = TestDataGenerator.create_test_files(
            test_dir, self.config.small_files, self.config
        )

        self._run_single_scan(
            "modification_initial",
            test_dir,
            db_path,
            files_info["total_files"],
            files_info["total_size"] / 1024 / 1024,
        )

        # ä¿®æ”¹éƒ¨åˆ†æ–‡ä»¶
        print("  ä¿®æ”¹ 20% çš„æ–‡ä»¶...")
        files_to_modify = int(files_info["total_files"] * 0.2)
        for i in range(files_to_modify):
            file_path = test_dir / f"file_{i:06d}.txt"
            if file_path.exists():
                try:
                    # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–å’Œä¿®æ”¹
                    content = (
                        file_path.read_text(encoding="utf-8") + "\nModified content"
                    )
                    file_path.write_text(content, encoding="utf-8")
                except UnicodeDecodeError:
                    # å¦‚æœæ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œåˆ™æ·»åŠ ä¸€äº›å­—èŠ‚
                    content = file_path.read_bytes()
                    content += b"\nModified binary content"
                    file_path.write_bytes(content)

        # é‡æ–°æ‰«æä¿®æ”¹åçš„æ–‡ä»¶
        print("  æ‰«æä¿®æ”¹åçš„æ–‡ä»¶...")
        modified_result = self._run_single_scan(
            "modification_after_changes",
            test_dir,
            db_path,
            files_info["total_files"],
            files_info["total_size"] / 1024 / 1024,
        )

        if modified_result:
            self.results.append(modified_result)

    def _run_single_scan(
        self,
        test_name: str,
        scan_dir: Path,
        db_path: Path,
        file_count: int,
        total_size_mb: float,
    ) -> Optional[PerformanceResult]:
        """è¿è¡Œå•æ¬¡æ‰«ææµ‹è¯•"""
        monitor = ResourceMonitor()

        # æ„å»º CLI å‘½ä»¤
        cmd = [
            "uv",
            "run",
            "python",
            "pyFileIndexer/main.py",
            str(scan_dir),
            "--machine_name",
            self.config.machine_name,
            "--db_path",
            str(db_path),
            "--log_path",
            str(self.temp_dir / f"{test_name}.log"),
        ]

        try:
            # å¼€å§‹ç›‘æ§
            monitor.start()
            start_time = time.time()

            # è¿è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            )

            end_time = time.time()
            resource_metrics = monitor.stop()

            if result.returncode != 0:
                print("    é”™è¯¯: CLI å‘½ä»¤æ‰§è¡Œå¤±è´¥")
                print(f"    stdout: {result.stdout}")
                print(f"    stderr: {result.stderr}")
                return None

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            scan_time = end_time - start_time
            files_per_second = file_count / scan_time if scan_time > 0 else 0
            mb_per_second = total_size_mb / scan_time if scan_time > 0 else 0

            # è·å–æ•°æ®åº“ä¿¡æ¯
            db_size_mb = 0
            db_records = 0
            if db_path.exists():
                db_size_mb = db_path.stat().st_size / 1024 / 1024
                db_records = self._count_db_records(db_path)

            return PerformanceResult(
                test_name=test_name,
                file_count=file_count,
                total_size_mb=total_size_mb,
                scan_time_seconds=scan_time,
                files_per_second=files_per_second,
                mb_per_second=mb_per_second,
                db_size_mb=db_size_mb,
                db_records=db_records,
                resource_metrics=resource_metrics,
                cli_output=result.stdout + result.stderr,
            )

        except subprocess.TimeoutExpired:
            monitor.stop()
            print("    é”™è¯¯: æµ‹è¯•è¶…æ—¶")
            return None
        except Exception as e:
            monitor.stop()
            print(f"    é”™è¯¯: {e}")
            return None

    def _count_db_records(self, db_path: Path) -> int:
        """ç»Ÿè®¡æ•°æ®åº“è®°å½•æ•°"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM file_meta")
            count = cursor.fetchone()[0]
            conn.close()
            return count
        except Exception:
            return 0


class BenchmarkReporter:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    @staticmethod
    def generate_report(results: List[PerformanceResult], output_dir: Path):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆ JSON æŠ¥å‘Š
        json_path = output_dir / "benchmark_results.json"
        BenchmarkReporter._generate_json_report(results, json_path)

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_path = output_dir / "benchmark_report.txt"
        BenchmarkReporter._generate_text_report(results, text_path)

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        summary_path = output_dir / "benchmark_summary.txt"
        BenchmarkReporter._generate_summary_report(results, summary_path)

        print("\nğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  JSON è¯¦ç»†æŠ¥å‘Š: {json_path}")
        print(f"  æ–‡æœ¬æŠ¥å‘Š: {text_path}")
        print(f"  æ±‡æ€»æŠ¥å‘Š: {summary_path}")

    @staticmethod
    def _generate_json_report(results: List[PerformanceResult], output_path: Path):
        """ç”Ÿæˆ JSON æ ¼å¼æŠ¥å‘Š"""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(results),
            "results": [asdict(result) for result in results],
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

    @staticmethod
    def _generate_text_report(results: List[PerformanceResult], output_path: Path):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("pyFileIndexer æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æµ‹è¯•æ•°: {len(results)}\n\n")

            for result in results:
                f.write(f"æµ‹è¯•: {result.test_name}\n")
                f.write("-" * 40 + "\n")
                f.write(f"æ–‡ä»¶æ•°é‡: {result.file_count:,}\n")
                f.write(f"æ€»å¤§å°: {result.total_size_mb:.2f} MB\n")
                f.write(f"æ‰«ææ—¶é—´: {result.scan_time_seconds:.2f} ç§’\n")
                f.write(f"å¤„ç†é€Ÿåº¦: {result.files_per_second:.1f} æ–‡ä»¶/ç§’\n")
                f.write(f"ååé‡: {result.mb_per_second:.2f} MB/ç§’\n")
                f.write(f"æ•°æ®åº“å¤§å°: {result.db_size_mb:.2f} MB\n")
                f.write(f"æ•°æ®åº“è®°å½•: {result.db_records:,}\n")

                if result.resource_metrics:
                    avg_cpu = sum(m.cpu_percent for m in result.resource_metrics) / len(
                        result.resource_metrics
                    )
                    avg_memory = sum(
                        m.memory_mb for m in result.resource_metrics
                    ) / len(result.resource_metrics)
                    f.write(f"å¹³å‡CPUä½¿ç”¨: {avg_cpu:.1f}%\n")
                    f.write(f"å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f} MB\n")

                f.write("\n")

    @staticmethod
    def _generate_summary_report(results: List[PerformanceResult], output_path: Path):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡æŠ¥å‘Š"""
        if not results:
            return

        with open(output_path, "w", encoding="utf-8") as f:
            f.write("æ€§èƒ½æµ‹è¯•æ±‡æ€»ç»Ÿè®¡\n")
            f.write("=" * 40 + "\n\n")

            # æŒ‰è§„æ¨¡åˆ†ç»„ç»Ÿè®¡
            scale_groups = {}
            for result in results:
                if "small" in result.test_name:
                    scale = "small"
                elif "medium" in result.test_name:
                    scale = "medium"
                elif "large" in result.test_name:
                    scale = "large"
                else:
                    scale = "other"

                if scale not in scale_groups:
                    scale_groups[scale] = []
                scale_groups[scale].append(result)

            for scale, group_results in scale_groups.items():
                if not group_results:
                    continue

                f.write(f"{scale.upper()} è§„æ¨¡æµ‹è¯•ç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")

                avg_files_per_sec = sum(
                    r.files_per_second for r in group_results
                ) / len(group_results)
                avg_mb_per_sec = sum(r.mb_per_second for r in group_results) / len(
                    group_results
                )
                avg_scan_time = sum(r.scan_time_seconds for r in group_results) / len(
                    group_results
                )

                f.write(f"å¹³å‡å¤„ç†é€Ÿåº¦: {avg_files_per_sec:.1f} æ–‡ä»¶/ç§’\n")
                f.write(f"å¹³å‡ååé‡: {avg_mb_per_sec:.2f} MB/ç§’\n")
                f.write(f"å¹³å‡æ‰«ææ—¶é—´: {avg_scan_time:.2f} ç§’\n")
                f.write(f"æµ‹è¯•è½®æ•°: {len(group_results)}\n\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="pyFileIndexer æ€§èƒ½æµ‹è¯•å·¥å…·")

    parser.add_argument(
        "--small", type=int, default=100, help="å°è§„æ¨¡æµ‹è¯•æ–‡ä»¶æ•°é‡ (é»˜è®¤: 100)"
    )
    parser.add_argument(
        "--medium", type=int, default=1000, help="ä¸­è§„æ¨¡æµ‹è¯•æ–‡ä»¶æ•°é‡ (é»˜è®¤: 1000)"
    )
    parser.add_argument(
        "--large", type=int, default=5000, help="å¤§è§„æ¨¡æµ‹è¯•æ–‡ä»¶æ•°é‡ (é»˜è®¤: 5000)"
    )
    parser.add_argument(
        "--rounds", type=int, default=3, help="æ¯ä¸ªæµ‹è¯•çš„è¿è¡Œè½®æ•° (é»˜è®¤: 3)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="benchmark_results",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: benchmark_results)",
    )
    parser.add_argument(
        "--machine-name",
        type=str,
        default="benchmark-test",
        help="æœºå™¨åç§°æ ‡è¯† (é»˜è®¤: benchmark-test)",
    )

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        small_files=args.small,
        medium_files=args.medium,
        large_files=args.large,
        test_rounds=args.rounds,
        machine_name=args.machine_name,
    )

    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    runner = BenchmarkRunner(config)
    results = runner.run_all_benchmarks()

    # ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(args.output)
    BenchmarkReporter.generate_report(results, output_dir)

    print(f"\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ! å…±è¿è¡Œ {len(results)} ä¸ªæµ‹è¯•")
    if results:
        total_files = sum(r.file_count for r in results)
        avg_speed = sum(r.files_per_second for r in results) / len(results)
        print(f"æ€»å¤„ç†æ–‡ä»¶: {total_files:,}")
        print(f"å¹³å‡å¤„ç†é€Ÿåº¦: {avg_speed:.1f} æ–‡ä»¶/ç§’")


if __name__ == "__main__":
    main()
